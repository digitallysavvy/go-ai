# Caching Middleware Example

Intelligent response caching for AI operations to reduce costs, improve performance, and handle rate limits.

## Features

- ✅ In-memory caching with TTL (Time-To-Live)
- ✅ Automatic cache key generation from request parameters
- ✅ Configurable cacheability rules
- ✅ Cache statistics (hits, misses, tokens saved, cost saved)
- ✅ LRU eviction when cache is full
- ✅ Automatic cleanup of expired entries
- ✅ File-based persistence (optional)
- ✅ Thread-safe operations
- ✅ Cost tracking and savings calculation

## Why Cache AI Responses?

### Cost Savings
- GPT-4: $0.03/1K input tokens, $0.06/1K output tokens
- Caching identical requests saves 100% of API costs
- Can save thousands of dollars at scale

### Performance
- Cache hits respond instantly (no network latency)
- Typical API calls: 1-3 seconds → Cache hits: <1ms
- Better user experience with instant responses

### Rate Limiting
- Stay within API rate limits
- Handle traffic spikes without errors
- Reduce load on AI provider

## Prerequisites

- Go 1.21 or higher
- OpenAI API key

## Setup

```bash
export OPENAI_API_KEY=sk-...
go run main.go
```

## Cache Strategies

### 1. Memory Cache

Fast, in-memory caching with TTL:

```go
cache := NewMemoryCache(
    5*time.Minute,  // TTL: entries expire after 5 minutes
    100,            // Max size: keep 100 entries
)
```

### 2. File Cache

Persistent caching across restarts:

```go
cache, _ := NewFileCache(
    "cache.json",
    1*time.Hour,
    1000,
)
```

### 3. Custom Cache

Implement your own (Redis, Memcached, etc.):

```go
type Cache interface {
    Get(key string) (*CacheEntry, bool)
    Set(key string, entry *CacheEntry)
    Delete(key string)
    Clear()
    Stats() CacheStats
}
```

## Cacheability Rules

Not all requests should be cached. Default rules:

```go
func defaultCacheable(opts ai.GenerateTextOptions) bool {
    // ✗ Don't cache tool-using requests (non-deterministic)
    if len(opts.Tools) > 0 {
        return false
    }

    // ✗ Don't cache high-temperature requests (creative, varied)
    if opts.Temperature != nil && *opts.Temperature > 0.5 {
        return false
    }

    // ✓ Cache everything else
    return true
}
```

### When to Cache

✅ **Do cache:**
- Factual queries (temperature ≤ 0.5)
- Repeated identical requests
- Reference documentation lookups
- Translation of fixed content
- Data extraction with consistent format

✗ **Don't cache:**
- Creative writing (high temperature)
- Tool-using agents (non-deterministic)
- Time-sensitive queries
- User-specific personalized responses
- Streaming requests

## Cache Key Generation

Keys are generated by hashing request parameters:

```go
func defaultKeyFunc(opts ai.GenerateTextOptions) string {
    h := sha256.New()
    h.Write([]byte(opts.Model.ModelID()))
    h.Write([]byte(opts.Prompt))
    h.Write([]byte(opts.System))

    if opts.Temperature != nil {
        h.Write([]byte(fmt.Sprintf("%f", *opts.Temperature)))
    }

    return hex.EncodeToString(h.Sum(nil))
}
```

This ensures identical requests get the same key.

## Code Highlights

### Basic Usage

```go
cache := NewMemoryCache(5*time.Minute, 100)
middleware := NewCachingMiddleware(cache)

result, err := middleware.GenerateText(ctx, ai.GenerateTextOptions{
    Model:       model,
    Prompt:      "What is the capital of France?",
    Temperature: &temp,
})
```

### Custom Cacheability

```go
middleware.cacheable = func(opts ai.GenerateTextOptions) bool {
    // Only cache prompts starting with "Translate:"
    return strings.HasPrefix(opts.Prompt, "Translate:")
}
```

### Custom Key Function

```go
middleware.keyFunc = func(opts ai.GenerateTextOptions) string {
    // Simple key: just hash the prompt
    h := sha256.New()
    h.Write([]byte(opts.Prompt))
    return hex.EncodeToString(h.Sum(nil))
}
```

## Cache Statistics

Track performance with built-in stats:

```go
stats := cache.Stats()
fmt.Printf("Hits: %d\n", stats.Hits)
fmt.Printf("Misses: %d\n", stats.Misses)
fmt.Printf("Hit Rate: %.1f%%\n", hitRate)
fmt.Printf("Tokens Saved: %d\n", stats.TotalTokensSaved)
fmt.Printf("Cost Saved: $%.4f\n", stats.CostSaved)
```

Example output:
```
Cache Hits:          5
Cache Misses:        3
Hit Rate:            62.5%
Tokens Saved:        487
Estimated Cost Saved: $0.0243
```

## Use Cases

### 1. FAQ Bot

Cache common questions:

```go
questions := []string{
    "What are your hours?",
    "How do I reset my password?",
    "What are your hours?",  // Cache hit!
}
```

### 2. Documentation Lookup

Cache documentation queries:

```go
temp := 0.0  // Deterministic
result, _ := middleware.GenerateText(ctx, ai.GenerateTextOptions{
    Model:       model,
    Prompt:      "Explain Go interfaces",
    Temperature: &temp,
})
```

### 3. Translation Service

Cache translations:

```go
// Same text translated multiple times
for _, userID := range users {
    middleware.GenerateText(ctx, ai.GenerateTextOptions{
        Prompt: "Translate to Spanish: Hello, how are you?",
    })
}
```

### 4. Data Extraction

Cache extraction from repeated documents:

```go
middleware.GenerateText(ctx, ai.GenerateTextOptions{
    Prompt: "Extract email addresses from: " + document,
    System: "Extract only email addresses as JSON array",
})
```

## Advanced Patterns

### Redis Cache

```go
import "github.com/go-redis/redis/v8"

type RedisCache struct {
    client *redis.Client
    ttl    time.Duration
}

func (rc *RedisCache) Get(key string) (*CacheEntry, bool) {
    data, err := rc.client.Get(ctx, key).Bytes()
    if err != nil {
        return nil, false
    }

    var entry CacheEntry
    json.Unmarshal(data, &entry)
    return &entry, true
}

func (rc *RedisCache) Set(key string, entry *CacheEntry) {
    data, _ := json.Marshal(entry)
    rc.client.Set(ctx, key, data, rc.ttl)
}
```

### Distributed Cache with Memcached

```go
import "github.com/bradfitz/gomemcache/memcache"

type MemcachedCache struct {
    client *memcache.Client
}

func (mc *MemcachedCache) Get(key string) (*CacheEntry, bool) {
    item, err := mc.client.Get(key)
    if err != nil {
        return nil, false
    }

    var entry CacheEntry
    json.Unmarshal(item.Value, &entry)
    return &entry, true
}
```

### Semantic Caching

Cache based on semantic similarity, not exact matches:

```go
type SemanticCache struct {
    embeddings map[string][]float64
    entries    map[string]*CacheEntry
    threshold  float64
}

func (sc *SemanticCache) Get(key string) (*CacheEntry, bool) {
    embedding := getEmbedding(key)

    for cachedKey, cachedEmbedding := range sc.embeddings {
        similarity := cosineSimilarity(embedding, cachedEmbedding)
        if similarity > sc.threshold {
            return sc.entries[cachedKey], true
        }
    }

    return nil, false
}
```

### Cache Warming

Pre-populate cache with common queries:

```go
func warmCache(middleware *CachingMiddleware, queries []string) {
    for _, query := range queries {
        middleware.GenerateText(ctx, ai.GenerateTextOptions{
            Prompt: query,
        })
    }
}

commonQueries := []string{
    "What are your business hours?",
    "How do I contact support?",
    "What is your refund policy?",
}

warmCache(middleware, commonQueries)
```

## Cache Management

### Clear Cache

```go
cache.Clear()
```

### Delete Specific Entry

```go
key := middleware.keyFunc(opts)
cache.Delete(key)
```

### Inspect Cache

```go
stats := cache.Stats()
fmt.Printf("Current size: %d entries\n", len(cache.entries))
```

## Performance Tuning

### TTL Selection

```go
// Short TTL (1-5 minutes): Frequently changing data
cache := NewMemoryCache(2*time.Minute, 100)

// Medium TTL (1 hour): Stable reference data
cache := NewMemoryCache(1*time.Hour, 500)

// Long TTL (24 hours): Static content
cache := NewMemoryCache(24*time.Hour, 1000)
```

### Cache Size

```go
// Small (100): Personal apps, low traffic
cache := NewMemoryCache(ttl, 100)

// Medium (1000): Team apps, moderate traffic
cache := NewMemoryCache(ttl, 1000)

// Large (10000): Production apps, high traffic
cache := NewMemoryCache(ttl, 10000)
```

## Testing

Run the example:

```bash
go run main.go
```

Expected behavior:
- First request: cache miss (actual API call)
- Repeated requests: cache hit (instant response)
- Statistics show tokens/cost saved

## Monitoring

### Prometheus Metrics

```go
import "github.com/prometheus/client_golang/prometheus"

var (
    cacheHits = prometheus.NewCounter(prometheus.CounterOpts{
        Name: "ai_cache_hits_total",
    })

    cacheMisses = prometheus.NewCounter(prometheus.CounterOpts{
        Name: "ai_cache_misses_total",
    })
)

func (c *MemoryCache) Get(key string) (*CacheEntry, bool) {
    entry, hit := // ...

    if hit {
        cacheHits.Inc()
    } else {
        cacheMisses.Inc()
    }

    return entry, hit
}
```

## Best Practices

1. **Set appropriate TTLs**: Balance freshness vs. cost savings
2. **Monitor hit rates**: Aim for >50% hit rate
3. **Don't cache everything**: Only deterministic queries
4. **Use semantic caching**: For similar but not identical queries
5. **Warm cache**: Pre-populate with common queries
6. **Monitor costs**: Track actual savings
7. **Consider privacy**: Don't cache sensitive data
8. **Implement eviction**: LRU or LFU for limited memory

## Troubleshooting

### Low Hit Rate

- Check if temperature is too high
- Verify requests are identical
- Consider semantic caching for similar queries

### High Memory Usage

- Reduce cache size
- Shorten TTL
- Implement more aggressive eviction

### Stale Responses

- Reduce TTL
- Implement cache invalidation
- Add timestamps to cache entries

## Next Steps

- Implement Redis/Memcached for distributed caching
- Add semantic similarity matching
- Create cache warming strategies
- Build cache analytics dashboard
- Implement tiered caching (memory + Redis)

## Related Examples

- [logging](../logging) - Logging middleware
- [rate-limiting](../rate-limiting) - Rate limiting middleware
- [middleware](../../middleware) - Basic middleware patterns

## Resources

- [Caching Strategies](https://aws.amazon.com/caching/best-practices/)
- [Redis Documentation](https://redis.io/documentation)
- [Cache Invalidation](https://martinfowler.com/bliki/TwoHardThings.html)
